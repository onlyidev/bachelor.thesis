@misc{al-dujailiAdversarialDeepLearning2018,
  title = {Adversarial {{Deep Learning}} for {{Robust Detection}} of {{Binary Encoded Malware}}},
  author = {{Al-Dujaili}, Abdullah and Huang, Alex and Hemberg, Erik and O'Reilly, Una-May},
  year = {2018},
  month = mar,
  number = {arXiv:1801.02950},
  eprint = {1801.02950},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.02950},
  urldate = {2025-02-28},
  abstract = {Malware is constantly adapting in order to avoid detection. Model based malware detectors, such as SVM and neural networks, are vulnerable to so-called adversarial examples which are modest changes to detectable malware that allows the resulting malware to evade detection. Continuous-valued methods that are robust to adversarial examples of images have been developed using saddle-point optimization formulations. We are inspired by them to develop similar methods for the discrete, e.g. binary, domain which characterizes the features of malware. A specific extra challenge of malware is that the adversarial examples must be generated in a way that preserves their malicious functionality. We introduce methods capable of generating functionally preserved adversarial malware examples in the binary domain. Using the saddle-point formulation, we incorporate the adversarial examples into the training of models that are robust to them. We evaluate the effectiveness of the methods and others in the literature on a set of Portable Execution{\textasciitilde}(PE) files. Comparison prompts our introduction of an online measure computed during training to assess general expectation of robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {SLEIPNIR dataset},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\MBWGQXDJ\\Al-Dujaili et al. - 2018 - Adversarial Deep Learning for Robust Detection of Binary Encoded Malware.pdf;C\:\\Users\\liuda\\Zotero\\storage\\S855L93U\\1801.html}
}

@misc{andersonEMBEROpenDataset2018a,
  title = {{{EMBER}}: {{An Open Dataset}} for {{Training Static PE Malware Machine Learning Models}}},
  shorttitle = {{{EMBER}}},
  author = {Anderson, Hyrum S. and Roth, Phil},
  year = {2018},
  month = apr,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.1804.04637},
  urldate = {2024-11-17},
  abstract = {This paper describes EMBER: a labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign). To accompany the dataset, we also release open source code for extracting features from additional binaries so that additional sample features can be appended to the dataset. This dataset fills a void in the information security machine learning community: a benign/malicious dataset that is large, open and general enough to cover several interesting use cases. We enumerate several use cases that we considered when structuring the dataset. Additionally, we demonstrate one use case wherein we compare a baseline gradient boosted decision tree model trained using LightGBM with default settings to MalConv, a recently published end-to-end (featureless) deep learning model for malware detection. Results show that even without hyper-parameter optimization, the baseline EMBER model outperforms MalConv. The authors hope that the dataset, code and baseline model provided by EMBER will help invigorate machine learning research for malware detection, in much the same way that benchmark datasets have advanced computer vision research.},
  keywords = {Computer Science - Cryptography and Security},
  annotation = {ADS Bibcode: 2018arXiv180404637A},
  file = {C:\Users\liuda\Zotero\storage\LGGKASMQ\Anderson and Roth - 2018 - EMBER An Open Dataset for Training Static PE Malware Machine Learning Models.pdf}
}

@article{chakrabortySurveyAdversarialAttacks2021,
  title = {A Survey on Adversarial Attacks and Defences},
  author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  year = {2021},
  journal = {CAAI Transactions on Intelligence Technology},
  volume = {6},
  number = {1},
  pages = {25--45},
  issn = {2468-2322},
  doi = {10.1049/cit2.12028},
  urldate = {2025-04-07},
  abstract = {Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human-level performance. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.},
  langid = {english},
  keywords = {deep learning (artificial intelligence),security of data},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\5GF5TPIQ\\Chakraborty et al. - 2021 - A survey on adversarial attacks and defences.pdf;C\:\\Users\\liuda\\Zotero\\storage\\FFL4BTFX\\cit2.html}
}

@article{demetrioAdversarialEXEmplesSurvey2021,
  title = {Adversarial {{EXEmples}}: {{A Survey}} and {{Experimental Evaluation}} of {{Practical Attacks}} on {{Machine Learning}} for {{Windows Malware Detection}}},
  shorttitle = {Adversarial {{EXEmples}}},
  author = {Demetrio, Luca and Coull, Scott E. and Biggio, Battista and Lagorio, Giovanni and Armando, Alessandro and Roli, Fabio},
  year = {2021},
  month = sep,
  journal = {ACM Trans. Priv. Secur.},
  volume = {24},
  number = {4},
  pages = {27:1--27:31},
  issn = {2471-2566},
  doi = {10.1145/3473039},
  urldate = {2024-09-30},
  abstract = {Recent work has shown that adversarial Windows malware samples---referred to as adversarial EXEmples in this article---can bypass machine learning-based detection relying on static code analysis by perturbing relatively few input bytes. To preserve malicious functionality, previous attacks either add bytes to existing non-functional areas of the file, potentially limiting their effectiveness, or require running computationally demanding validation steps to discard malware variants that do not correctly execute in sandbox environments. In this work, we overcome these limitations by developing a unifying framework that does not only encompass and generalize previous attacks against machine-learning models, but also includes three novel attacks based on practical, functionality-preserving manipulations to the Windows Portable Executable file format. These attacks, named Full DOS, Extend, and Shift, inject the adversarial payload by respectively manipulating the DOS header, extending it, and shifting the content of the first section. Our experimental results show that these attacks outperform existing ones in both white-box and black-box scenarios, achieving a better tradeoff in terms of evasion rate and size of the injected payload, while also enabling evasion of models that have been shown to be robust to previous attacks. To facilitate reproducibility of our findings, we open source our framework and all the corresponding attack implementations as part of the secml-malware Python library. We conclude this work by discussing the limitations of current machine learning-based malware detectors, along with potential mitigation strategies based on embedding domain knowledge coming from subject-matter experts directly into the learning process.},
  keywords = {10},
  file = {C:\Users\liuda\Zotero\storage\LLWZBM8T\Demetrio et al. - 2021 - Adversarial EXEmples A Survey and Experimental Evaluation of Practical Attacks on Machine Learning.pdf}
}

@misc{huGeneratingAdversarialMalware2017,
  title = {Generating {{Adversarial Malware Examples}} for {{Black-Box Attacks Based}} on {{GAN}}},
  author = {Hu, Weiwei and Tan, Ying},
  year = {2017},
  month = feb,
  number = {arXiv:1702.05983},
  eprint = {1702.05983},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms.Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples' malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {5,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\liuda\Zotero\storage\W36MX2CP\Hu and Tan - 2017 - Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN.pdf}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {1135--1144},
  publisher = {ACM},
  address = {San Francisco California USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2025-02-28},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  annotation = {LIME original}
}

@inproceedings{ribeiroWhyShouldTrust2016a,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {1135--1144},
  publisher = {ACM},
  address = {San Francisco California USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2025-04-15},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  file = {C:\Users\liuda\Zotero\storage\V8D3S42E\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf}
}

@article{tcydenovaDetectionAdversarialAttacks2021,
  title = {Detection of {{Adversarial Attacks}} in {{AI-Based Intrusion Detection Systems Using Explainable AI}}},
  author = {Tcydenova, Erzhena and Kim, Tae Woo and Lee, Changhoon and Park, Jong Hyuk},
  year = {2021},
  month = sep,
  journal = {Human-centric Computing and Information Sciences},
  volume = {11},
  number = {0},
  pages = {1--1},
  doi = {10.22967/HCIS.2021.11.035},
  urldate = {2025-02-28},
  abstract = {With the tremendous increase in networking devices connected to the Internet, network security is recognized as an important issue. Intrusion detection systems (IDSs) are one of the important components of network security. There are several methods for implementing an IDS, and one is machine learning. The machine learning performance of IDSs is evolving to a very large extent and is being used in real IDSs. However, recent studies showed that machine learning classification models are vulnerable to adversarial attacks. In this paper, we propose an adversarial attack detection framework in machine learning-based explainable AI intrusion detection systems. The proposed framework consists of two phases: initialization and detection. In the initialization phase, we train an IDS based on a support vector machine classification model and extract explanations of the Normal data records from the dataset using LIME (local interpretable model-agnostic explanations). Based on the resulting explanations, results of the classification by the trained IDS are analyzed during the detection phase by explanation to detect an adversarial attack. We evaluate the proposed method using the NSL-KDD dataset.},
  langid = {english},
  annotation = {IDS use LIME for adversarial detection},
  file = {C:\Users\liuda\Zotero\storage\A6IL6B28\Tcydenova et al. - 2021 - Detection of Adversarial Attacks in AI-Based Intrusion Detection Systems Using Explainable AI.pdf}
}
