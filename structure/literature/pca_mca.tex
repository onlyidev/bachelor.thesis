\subsection{Dimensijų mažinimo metodai}

\subsubsection{Principinių komponenčių analizė (\gls{pca})}

\gls{pca} -- tai daugiamatės statistinės analizės metodas, naudojamas išskirti svarbiausią informaciją iš turimų duomenų. Svarbiausios informacijos apibrėžimui naudojama \glsko{inertia} metrika. \Gls{inertia} -- tai dalis variacijos, kuri yra paaiškinama viena komponente. Iš esmės \gls{pca} yra bazės keitimo metodas, kurio tikslas yra parinkti bazę, tenkinančią nelygybę $\forall n: I_n \ge I_{n+1}$, kur $I_n$ -- $n$-osios komponentės \gls{inertia} \cite{abdiPrincipalComponentAnalysis2010}. Tai leidžia sumažinti naudojamų komponenčių skaičių pasirenkant pakankamą \glsko{inertia} vertę. Pavyzdžiui, jei pirmosios komponentės \gls{inertia} yra $50\%$, antrosios -- $25\%$ (\rightarrow \; likusių komponenčių \gls{inertia} lygi $25\%$) ir pakankama \glsko{inertia} vertė yra $70\%$, tuomet užtenka palikti pirmas dvi komponentes, kurių bendra \gls{inertia} \angl{cummulative inertia} yra $75\%$.

\subsubsection{Daugialypės korespondencijos analizė (\gls{mca})}

\Glsxtrfull{mca} -- tai dar vienas daugiamatės statistinės analizės metodas, tik šis skirtas kategoriniams duomenims. Šio metodo veikimo principas pagrįstas standartine korespondencijos analize \angl{correspondence analysis} iš kategorinių duomenų sukonstruotai indikatorių matricai. \gls{mca} gali būti laikomas \gls{pca} praplėtimu kategoriniams duomenims \cite{abdiMultipleCorrespondenceAnalysis2007}, tad ir dimensijų mažinimo procesas toks pat -- pasirenkama pakankama \glsko{inertia} vertė $\varepsilon$ ir pasirenkami pirmi $m$ stulpelių taip, kad $\sum_{i=1}^m{I_i} \ge \varepsilon$

% TODO: Write about training MCA like a model on training data, then using it to project any other value  
% 